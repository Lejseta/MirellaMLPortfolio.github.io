1. Business Understanding

The goal of this project is to predict customer churn (whether a customer will leave the company or not) using KNN. This is a classification problem where the target variable is either Yes or No.

2. Data Understanding

- The dataset contains information about customers, their service usage, and churn status.
- The 'Churn' column is the target variable, where:
  - Yes = 1 (customer resignation)
  - No = 0 (Customer retained)

Key Features:
- Numerical features: SeniorCitizen, tenure, MonthlyCharges, TotalCharges
- Categorical features: gender, Partner, Dependents, PhoneService, MultipleLines, InternetService, OnlineSecurity, DeviceProtection, TechSupport, StreamingTV, StreamingMovies,Contract, PaperlessBilling,  PaymentMethod
- Target variable: Churn

Data Issues Identified:
- 'customerID' column was removed as it does not provide predictive value.
- 'TotalCharges' was converted to numeric, and missing values were filled with median.

3. Data Preparation

 Pre-processing Steps
- Handling Missing Values: 'TotalCharges' missing values were replaced with median.
- Feature Encoding:
  - Categorical variables were transformed using OneHotEncoder.
  - Numerical variables were standardized using StandardScaler.
- Handling Class Imbalance:
  - Applied SMOTE to balance class distribution.

4. Modeling
Model Version (1)
Algorithm: K-Nearest Neighbors (KNN)

Accuracy: 69%

Classification Report:
               precision    recall  f1-score   support

           0       0.91      0.65      0.76      1035
           1       0.46      0.83      0.59       374

    accuracy                           0.69      1409
   macro avg       0.69      0.74      0.67      1409
weighted avg       0.79      0.69      0.71      1409

ROC-AUC Score: 0.807344545196207

Model (Version 2)
Algorithm: K-Nearest Neighbors (KNN)
Changes: 
- sampling_strategy=0.5 added to SMOTE
Accuracy: 76%
Classification Report:
               precision    recall  f1-score   support

           0       0.88      0.79      0.83      1035
           1       0.54      0.69      0.61       374

    accuracy                           0.76      1409
   macro avg       0.71      0.74      0.72      1409
weighted avg       0.79      0.76      0.77      1409

ROC-AUC Score: 0.8178395205249427

---------------------------------------------
*Random forest experiment results (with SMOTE):
Accuracy: 0.76%
Classification Report:
               precision    recall  f1-score   support

           0       0.87      0.80      0.83      1035
           1       0.54      0.67      0.60       374

    accuracy                           0.76      1409
   macro avg       0.71      0.73      0.72      1409
weighted avg       0.78      0.76      0.77      1409


-----------------------------------------------------------------


6. Deployment

- Saving the serialized version of a model
- Developing a Flask app that can handle incoming requests, process them and send the response with the prediction.
- Building an isolated application container for simple project initialization

Conclusion:

The end model identifies customers who leave better (higher recall), but losing slightly in precision, although the accuracy has not increased relative to the 1st version of model.